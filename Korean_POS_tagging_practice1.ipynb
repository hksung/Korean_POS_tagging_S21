{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkHa7V2y3oVd",
        "outputId": "6a224076-d4f5-4a2a-a11e-97f77669628f"
      },
      "source": [
        "'''\n",
        "Step 1. Upload the file and read\n",
        "'''\n",
        "\n",
        "data = open('ko_kaist-ud.txt', 'r', encoding='utf-8') # encoding='utf-8' : Korean data set\n",
        "\n",
        "lines = data.readlines() #read by lines\n",
        "data.close()\n",
        "\n",
        "len(lines) #31468\n",
        "lines[:20] #sample 20\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1\\t내\\t내\\tADJ\\tmma\\t_\\t2\\tamod\\t_\\t_\\n',\n",
              " '2\\t고향은\\t고향+은\\tNOUN\\tncn+jxt\\t_\\t3\\tdislocated\\t_\\t_\\n',\n",
              " '3\\t서울입니다\\t서울+이+ㅂ니다\\tVERB\\tnq+jp+ef\\t_\\t0\\troot\\t_\\tSpaceAfter=No\\n',\n",
              " '4\\t.\\t.\\tPUNCT\\tsf\\t_\\t3\\tpunct\\t_\\t_\\n',\n",
              " '1\\t옛날의\\t옛날+의\\tNOUN\\tncn+jcm\\t_\\t2\\tnmod\\t_\\t_\\n',\n",
              " '2\\t서울의\\t서울+의\\tPROPN\\tnq+jcm\\t_\\t3\\tnmod\\t_\\t_\\n',\n",
              " '3\\t모습이\\t모습+이\\tNOUN\\tncn+jcs\\t_\\t5\\tnsubj\\t_\\t_\\n',\n",
              " '4\\t몹시\\t몹시\\tADV\\tmag\\t_\\t5\\tadvmod\\t_\\t_\\n',\n",
              " '5\\t그립습니다\\t그립+습니다\\tADJ\\tpaa+ef\\t_\\t0\\troot\\t_\\tSpaceAfter=No\\n',\n",
              " '6\\t.\\t.\\tPUNCT\\tsf\\t_\\t5\\tpunct\\t_\\t_\\n',\n",
              " '1\\t이른\\t이르+ㄴ\\tADJ\\tpaa+etm\\t_\\t2\\tamod\\t_\\t_\\n',\n",
              " '2\\t아침과\\t아침+과\\tCCONJ\\tncn+jcj\\t_\\t4\\tdep\\t_\\t_\\n',\n",
              " '3\\t밤\\t밤\\tNOUN\\tncn\\t_\\t2\\tconj\\t_\\t_\\n',\n",
              " '4\\t늦게\\t늦+게\\tSCONJ\\tpaa+ecs\\t_\\t12\\txcomp\\t_\\t_\\n',\n",
              " '5\\t종로\\t종로\\tPROPN\\tnq\\t_\\t7\\tnmod\\t_\\t_\\n',\n",
              " '6\\t보신각의\\t보신각+의\\tPROPN\\tnq+jcm\\t_\\t5\\tflat\\t_\\t_\\n',\n",
              " '7\\t종소리를\\t종소리+를\\tNOUN\\tncn+jco\\t_\\t8\\tobj\\t_\\t_\\n',\n",
              " '8\\t듣고\\t듣+고\\tCCONJ\\tpvg+ecc\\t_\\t14\\tacl\\t_\\t_\\n',\n",
              " '9\\t잠을\\t잠+을\\tNOUN\\tncn+jco\\t_\\t10\\tobj\\t_\\t_\\n',\n",
              " '10\\t자고\\t자+고\\tCCONJ\\tpvg+ecc\\t_\\t8\\tconj\\t_\\t_\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pym53fcI4CPd"
      },
      "source": [
        "index = []\n",
        "word = []\n",
        "pos = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    line = line.strip() # delete '\\n'\n",
        "    splits = line.split('\\t') # split by '\\t'\n",
        "\n",
        "    if len(splits) > 2: # filter '\\n\\'\n",
        "        try:\n",
        "            idx = int(splits[0])\n",
        "            index.append(idx)\n",
        "            word.append(splits[1])\n",
        "            pos.append(splits[3])\n",
        "\n",
        "        except:\n",
        "            print(i, index[-5:], word[-5:], pos[-5:])\n",
        "            print(line) #check if there is an error in idx\n",
        "            # or use print(set(index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2QJ145p73r2",
        "outputId": "ba82cc7e-78be-4403-9e25-6f9cd4283fa4"
      },
      "source": [
        "print (index[:20])\n",
        "print (word[:20])\n",
        "print (pos[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "['내', '고향은', '서울입니다', '.', '옛날의', '서울의', '모습이', '몹시', '그립습니다', '.', '이른', '아침과', '밤', '늦게', '종로', '보신각의', '종소리를', '듣고', '잠을', '자고']\n",
            "['ADJ', 'NOUN', 'VERB', 'PUNCT', 'NOUN', 'PROPN', 'NOUN', 'ADV', 'ADJ', 'PUNCT', 'ADJ', 'CCONJ', 'NOUN', 'SCONJ', 'PROPN', 'PROPN', 'NOUN', 'CCONJ', 'NOUN', 'CCONJ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWablQWF88-e",
        "outputId": "024bd03e-51c8-43eb-d241-dd6facfb6130"
      },
      "source": [
        "'''\n",
        "Step 2. Formating the data\n",
        "- Represent the dataset as a list of dictionaries\n",
        "- Represent words as dictionaries with multiple features\n",
        "'''\n",
        "\n",
        "input_data = [] #total sentences\n",
        "\n",
        "sent = [] #make a sentence->input_data\n",
        "i = 0\n",
        "pos_index = 0\n",
        "\n",
        "while (i < len(index)):\n",
        "    if (int(index[i])) == pos_index + 1:\n",
        "          sent.append({\"word\" :word[i], \"pos\" :pos[i]})\n",
        "          pos_index += 1\n",
        "          i = i +1\n",
        "    else:\n",
        "        if sent:\n",
        "            input_data.append(sent) #to filterout empty index\n",
        "        else:\n",
        "            i = i +1\n",
        "        pos_index = 0 #new sentence, make the number of sentence to default number\n",
        "        sent = []\n",
        "        if i == (len(index)-1):\n",
        "            break\n",
        "    \n",
        "    if i % 10000 == 0:\n",
        "        print (i, len(index)) #check an error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 25278\n",
            "20000 25278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJmyF4hEA4JA",
        "outputId": "eb879f15-18df-4893-cb38-c084e65378ab"
      },
      "source": [
        "len(input_data) #the number of total sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGQXCPh0A_fF",
        "outputId": "f0d237e3-4c43-4f7f-8a8c-4a0d3915863a"
      },
      "source": [
        "full_data = input_data\n",
        "print (full_data[1][:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'word': '옛날의', 'pos': 'NOUN'}, {'word': '서울의', 'pos': 'PROPN'}, {'word': '모습이', 'pos': 'NOUN'}, {'word': '몹시', 'pos': 'ADV'}, {'word': '그립습니다', 'pos': 'ADJ'}, {'word': '.', 'pos': 'PUNCT'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQ0eF8wBQ81"
      },
      "source": [
        "'''\n",
        "Step 3. Format the data for scikit-learn and extract the features\n",
        "'''\n",
        "\n",
        "\n",
        "def simple_features(input_sent,idx,token): #takes a sentence as input (with word and tag specified), outputs a more feature-rich version\n",
        "\tfeatures = {}\n",
        "\t#features[\"word\"] = token[\"word\"]\n",
        "\tif idx == 0:\n",
        "\t\tfeatures[\"prev_pos\"] = \"<start>\" #no previous pos\n",
        "\n",
        "\telif idx == 1:\n",
        "\t\tfeatures[\"prev_pos\"] = input_sent[idx-1][\"pos\"] #previos pos_tag\n",
        "\n",
        "\telse:\n",
        "\t\tfeatures[\"prev_pos\"] = input_sent[idx-1][\"pos\"] #\n",
        "\n",
        "\treturn(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVq-Qup0CFJP"
      },
      "source": [
        "def feature_extractor(input_data): #takes list [sents] of lists [tokens] of dictionaries [token_features], outputs a flat list of dicts [features]\n",
        "\tfeature_list = [] #flast list of token dictionaries\n",
        "\tfor sent in input_data: #iterate through sentences\n",
        "\t\tfor idx, token in enumerate(sent): #iterate through tokens\n",
        "\t\t\tfeature_list.append(simple_features(sent,idx,token)) #use simple_features function to add features\n",
        "\treturn(feature_list)\n",
        "\n",
        "def extract_pos(input_data):\n",
        "\tpos_list = []\n",
        "\tfor sent in input_data:\n",
        "\t\tfor token in sent:\n",
        "\t\t\tpos_list.append(token[\"pos\"])\n",
        "\treturn(pos_list)\n",
        "\n",
        "def extract_words(input_data):\n",
        "\tword_list = []\n",
        "\tfor sent in input_data:\n",
        "\t\tfor token in sent:\n",
        "\t\t\tword_list.append(token[\"word\"])\n",
        "\treturn(word_list)\n",
        "\n",
        "flat_words = extract_words(full_data)\n",
        "flat_pos = extract_pos(full_data)\n",
        "flat_features = feature_extractor(full_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRwpvKMHDNKx",
        "outputId": "b85d2135-7be7-427b-b5a1-f530a08dd530"
      },
      "source": [
        "for idx, x in enumerate(flat_words[:10]):\n",
        "  print(x, flat_pos[idx], flat_features[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "내 ADJ {'prev_pos': '<start>'}\n",
            "고향은 NOUN {'prev_pos': 'ADJ'}\n",
            "서울입니다 VERB {'prev_pos': 'NOUN'}\n",
            ". PUNCT {'prev_pos': 'VERB'}\n",
            "옛날의 NOUN {'prev_pos': '<start>'}\n",
            "서울의 PROPN {'prev_pos': 'NOUN'}\n",
            "모습이 NOUN {'prev_pos': 'PROPN'}\n",
            "몹시 ADV {'prev_pos': 'NOUN'}\n",
            "그립습니다 ADJ {'prev_pos': 'ADV'}\n",
            ". PUNCT {'prev_pos': 'ADJ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BsigLNODZzI",
        "outputId": "054d1431-c0af-4fb6-a7fc-92d1ae8ca4bc"
      },
      "source": [
        "print (len(flat_words) * .67) #16928\n",
        "\n",
        "#training data\n",
        "train_words = flat_words[:16928]\n",
        "train_pos = flat_pos[:16928]\n",
        "train_features = flat_features[:16928]\n",
        "\n",
        "#test data\n",
        "test_words = flat_words[16928:]\n",
        "test_pos = flat_pos[16928:]\n",
        "test_features = flat_features[16928:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16927.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUctA0CDrHs"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "vec = DictVectorizer(sparse = True)\n",
        "\n",
        "#transform categorical variables to vectors\n",
        "\n",
        "#we use .fit_transform() to create the vectors\n",
        "train_features_vec = vec.fit_transform(train_features) #vectorize sample of features\n",
        "\n",
        "#and apply previously made vectors using .transform()\n",
        "test_features_vec = vec.transform(test_features)\n",
        "\n",
        "#create our own POS conversion dictionary\n",
        "def pos_cats(pos_list):\n",
        "\tcat_d = {}\n",
        "\tfor idx, x in enumerate(list(set(pos_list))):\n",
        "\t\tcat_d[x] = idx\n",
        "\treturn(cat_d)\n",
        "\n",
        "pos_d = pos_cats(flat_pos)\n",
        "#print(pos_d)\n",
        "\n",
        "def convert_pos(pos_list,pos_d):\n",
        "\tconverted = []\n",
        "\tfor x in pos_list:\n",
        "\t\tconverted.append(pos_d[x])\n",
        "\treturn(converted)\n",
        "\n",
        "train_pos_num = convert_pos(train_pos,pos_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rk1LWV9Dzxl",
        "outputId": "caaf7504-3c44-4a1c-8d4a-53f5e857c11c"
      },
      "source": [
        "train_pos_num[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 16, 6, 9, 3, 9, 11, 8, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afal6JaGD3yc",
        "outputId": "4b5b787b-8374-4adc-a26f-db4e62cab18e"
      },
      "source": [
        "from sklearn import tree #import decision tree module\n",
        "\n",
        "clf = tree.DecisionTreeClassifier() #create classifier\n",
        "clf = clf.fit(train_features_vec,train_pos_num) #train model (features, pos tags)\n",
        "\n",
        "pred1 = clf.predict(test_features_vec) #apply model to new data\n",
        "\n",
        "print(pred1[:10]) #print first ten items in pred1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16  9  9  9  9  9  9  6  9  9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocBpibRED6jr",
        "outputId": "848356ac-d931-4f25-84a5-b49b7880964a"
      },
      "source": [
        "rev_pos_d = {value : key for (key, value) in pos_d.items()}\n",
        "\n",
        "def extract_pred_pos(pred_array,rev_d):\n",
        "\tpredicted = []\n",
        "\tfor x in pred_array:\n",
        "\t\tpredicted.append(rev_d[x])\n",
        "\treturn(predicted)\n",
        "\n",
        "pred1_pos = extract_pred_pos(pred1,rev_pos_d)\n",
        "pred1_pos[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VERB',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'PUNCT',\n",
              " 'NOUN',\n",
              " 'NOUN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQV2-XeqD9eJ",
        "outputId": "1389a97c-9784-4a1b-f8dc-c37786a0a4b5"
      },
      "source": [
        "def pred_accuracy(pred,gold):\n",
        "\tc = 0\n",
        "\tf = 0\n",
        "\n",
        "\tfor idx, x in enumerate(pred):\n",
        "\t\t#print(x,gold[idx][\"pos\"])\n",
        "\t\tif x == gold[idx]:\n",
        "\t\t\tc+=1\n",
        "\t\telse:\n",
        "\t\t\tf+=1\n",
        "\treturn(c/(c+f))\n",
        "\n",
        "pred_accuracy(pred1_pos,test_pos) #0.3284 ...???"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32841549718124025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubxu5c1vEC5L",
        "outputId": "76c770f5-d349-43a3-c5c2-ffdea7ba75fc"
      },
      "source": [
        "def simple_features2(input_sent,idx,token): #takes a sentence as input (with word and tag specified), outputs a more feature-rich version\n",
        "\tfeatures = {}\n",
        "\tfeatures[\"word\"] = token[\"word\"]\n",
        "\tif idx == 0:\n",
        "\t\tfeatures[\"prev_pos\"] = \"<start>\" #no previous pos\n",
        "\n",
        "\telif idx == 1:\n",
        "\t\tfeatures[\"prev_pos\"] = input_sent[idx-1][\"pos\"] #previos pos_tag\n",
        "\n",
        "\telse:\n",
        "\t\tfeatures[\"prev_pos\"] = input_sent[idx-1][\"pos\"] #\n",
        "\n",
        "\tfeatures[\"suffix_tg\"] = token[\"word\"][-3:] #get last three characters\n",
        "\n",
        "\treturn(features)\n",
        "\n",
        "def feature_extractor2(input_data): #takes list [sents] of lists [tokens] of dictionaries [token_features], outputs a flat list of dicts [features]\n",
        "\tfeature_list = [] #flast list of token dictionaries\n",
        "\tfor sent in input_data: #iterate through sentences\n",
        "\t\tfor idx, token in enumerate(sent): #iterate through tokens\n",
        "\t\t\tfeature_list.append(simple_features2(sent,idx,token)) #use simple_features function to add features\n",
        "\treturn(feature_list)\n",
        "\n",
        "flat_features2 = feature_extractor2(full_data)\n",
        "\n",
        "print(flat_features2[0])\n",
        "\n",
        "train_features2 = flat_features2[:16928]\n",
        "test_features2 = flat_features2[16928:]\n",
        "\n",
        "vec2 = DictVectorizer(sparse = True)\n",
        "\n",
        "#transform categorical variables to vectors\n",
        "\n",
        "#we use .fit_transform() to create the vectors\n",
        "train_features2_vec = vec2.fit_transform(train_features2) #vectorize sample of features\n",
        "\n",
        "#and apply previously made vectors using .transform()\n",
        "test_features2_vec = vec2.transform(test_features2)\n",
        "\n",
        "clf2 = tree.DecisionTreeClassifier()\n",
        "clf2 = clf2.fit(train_features2_vec,train_pos_num)\n",
        "\n",
        "pred2 = clf2.predict(test_features2_vec)\n",
        "pred2_pos = extract_pred_pos(pred2,rev_pos_d)\n",
        "#check accuracy\n",
        "print (pred_accuracy(pred2_pos,test_pos)) #0.6269"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'word': '내', 'prev_pos': '<start>', 'suffix_tg': '내'}\n",
            "0.6269641357802567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_unYnlK_Eexr",
        "outputId": "650d5376-34df-4c99-cf91-a5228e15a183"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}